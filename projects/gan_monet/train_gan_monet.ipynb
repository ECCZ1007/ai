{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args='find outputs -mindepth 1 -delete', returncode=0)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Third-party imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "import glob\n",
    "import os\n",
    "from PIL import Image\n",
    "from typing import Callable, Optional\n",
    "import IPython\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision.utils import save_image\n",
    "import subprocess\n",
    "\n",
    "# Device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Create outputs folders\n",
    "os.makedirs(\"outputs\", exist_ok=True)\n",
    "\n",
    "# Clean output folders\n",
    "subprocess.run(\"find outputs -mindepth 1 -delete\", shell=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and Model Hyperparameters\n",
    "lr = 3e-4\n",
    "batch_size = 16\n",
    "img_dim = 3 * 256 * 256\n",
    "num_epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\"Basic building block of the Generator and Discriminator networks.\n",
    "    \n",
    "    Consists of a linear layer, batch normalization and LeakyReLU activation.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels: int, out_channels: int, stride: int):\n",
    "        \"\"\"Initializes the Block.\n",
    "        \n",
    "        Args:\n",
    "            in_channels (int): Number of input features.\n",
    "            out_channels (int): Number of output features.\n",
    "            stride (int): Stride of conv block.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=4, stride=stride, padding=1, bias=True, padding_mode=\"reflect\"),\n",
    "            nn.InstanceNorm2d(out_channels),\n",
    "            nn.LeakyReLU(0.2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    \"\"\"Discriminator network.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels: int = 3, features: int = [64, 128, 256, 512]):\n",
    "        \"\"\"Initializes the Discriminator network.\n",
    "        \n",
    "        Args:\n",
    "            in_channels (int): Number of channels in the input image.\n",
    "            features (list): Number of features in each layer.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # Initial does not use instance norm in cycleGAN paper.\n",
    "        self.initial = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, features[0], kernel_size=4, stride=2, padding=1, padding_mode=\"reflect\"),\n",
    "            nn.LeakyReLU(0.2)\n",
    "        )\n",
    "        layers = []\n",
    "        in_channels = features[0]\n",
    "        for feature in features[1:]:\n",
    "            layers.append(Block(in_channels, feature, stride=1 if feature == features[-1] else 2))\n",
    "            in_channels = feature\n",
    "        layers.append(nn.Conv2d(in_channels, 1, kernel_size=4, stride=1, padding=1, padding_mode=\"reflect\"))\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.initial(x)\n",
    "        return torch.sigmoid(self.model(x))\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    \"\"\"Building block for generator.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, down=True, use_act=True, **kwargs):\n",
    "        \"\"\"Initializes the ConvBlock.\n",
    "        \n",
    "        Args:\n",
    "            in_channels (int): Number of input channels.\n",
    "            out_channels (int): Number of output channels.\n",
    "            down (bool): Whether to use a downsample or upsample.\n",
    "            use_act (bool): Whether to use an activation function.\n",
    "            **kwargs: Additional arguments for the convolutional layer.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, padding_mode=\"reflect\", **kwargs) if down else nn.ConvTranspose2d(in_channels, out_channels, **kwargs),\n",
    "            nn.InstanceNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True) if use_act else nn.Identity()\n",
    "        )\n",
    "        self.down = down\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    \"\"\"Residual block for generator.\n",
    "    \"\"\"\n",
    "    def __init__(self, channels):\n",
    "        \"\"\"Initializes the ResidualBlock.\n",
    "        \n",
    "        Args:\n",
    "            channels (int): Number of channels in the input and output.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            ConvBlock(channels, channels, kernel_size=3, stride=1, padding=1),\n",
    "            ConvBlock(channels, channels, use_act=False, kernel_size=3, stride=1, padding=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.block(x)\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    \"\"\"Generator network.\n",
    "    \n",
    "    Attempts to augoment an input image to look like a Monet painting.\n",
    "    \"\"\"\n",
    "    def __init__(self, img_channels: int = 3, num_features: int = 64, num_residuals: int = 9):\n",
    "        \"\"\"Initializes the Generator network.\n",
    "        \n",
    "        Args:\n",
    "            img_channels (int): Dimension of the image space.\n",
    "            num_features (int): Number of features to use.\n",
    "            num_residuals (int): Number of residual blocks to use.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.initial = nn.Sequential(\n",
    "            nn.Conv2d(img_channels, num_features, kernel_size=7, stride=1, padding=3),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.down_blocks = nn.ModuleList([\n",
    "            ConvBlock(num_features, num_features*2, down=True, kernel_size=3, stride=2, padding=1),\n",
    "            ConvBlock(num_features*2, num_features*4, down=True, kernel_size=3, stride=2, padding=1),\n",
    "        ])\n",
    "        self.residual_blocks = nn.Sequential(*[ResidualBlock(num_features*4) for _ in range(num_residuals)])\n",
    "        self.up_blocks = nn.ModuleList([\n",
    "            ConvBlock(num_features*4, num_features*2, down=False, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            ConvBlock(num_features*2, num_features, down=False, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "        ])\n",
    "        self.last = nn.Conv2d(num_features, img_channels, kernel_size=7, stride=1, padding=3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.initial(x)\n",
    "        for down_block in self.down_blocks:\n",
    "            x = down_block(x)\n",
    "        x = self.residual_blocks(x)\n",
    "        for up_block in self.up_blocks:\n",
    "            x = up_block(x)\n",
    "        return torch.tanh(self.last(x))\n",
    "\n",
    "# Init the models\n",
    "gen_monet = Generator().to(device)\n",
    "gen_raw = Generator().to(device)\n",
    "disc_monet = Discriminator().to(device)\n",
    "disc_raw = Discriminator().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datasets\n",
    "class ImageDataset(Dataset):\n",
    "    \"\"\"Image dataset.\n",
    "    \n",
    "    Loads images from a directory.\n",
    "    \"\"\"\n",
    "    def __init__(self, train_dir: str, test_dir: str, transforms: Optional[Callable] = None):\n",
    "        \"\"\"Initializes the ImageDataset.\n",
    "        \n",
    "        Args:\n",
    "            train_dir (str): Directory containing the raw training images.\n",
    "            test_dir (str): Directory containing the raw training images.\n",
    "            transforms (Optional[Callable]): Optional transform to be applied on a sample.\n",
    "        \"\"\"\n",
    "        self.train_dir = sorted(glob.glob(os.path.join(train_dir, \"*.jpg\")))\n",
    "        self.test_dir = sorted(glob.glob(os.path.join(test_dir, \"*.jpg\")))\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        train_image = Image.open(self.train_dir[index % len(self.train_dir)])\n",
    "        test_image = Image.open(self.test_dir[index % len(self.test_dir)])\n",
    "        if self.transforms:\n",
    "            train_image = self.transforms(train_image)\n",
    "            test_image = self.transforms(test_image)\n",
    "        return train_image, test_image\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.train_dir)\n",
    "# Image Transforms\n",
    "transforms = transforms.Compose([\n",
    "    transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "# Create the dataset\n",
    "ds = ImageDataset(train_dir='data/photo_jpg/', test_dir='data/monet_jpg/', transforms=transforms)\n",
    "\n",
    "# Create the data loader\n",
    "dataloader = DataLoader(ds, batch_size=batch_size, shuffle=True, num_workers=4)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/3519 [00:01<38:34,  1.52it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/50] Batch 0/3519                   Loss D: 0.5174, loss G: 1.8653\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 102/3519 [00:15<08:19,  6.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/50] Batch 100/3519                   Loss D: 0.5024, loss G: 1.2675\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 169/3519 [00:24<08:07,  6.87it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [5], line 57\u001b[0m\n\u001b[1;32m     55\u001b[0m opt_gen\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m     56\u001b[0m G_loss\u001b[39m.\u001b[39mbackward()\n\u001b[0;32m---> 57\u001b[0m opt_gen\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     59\u001b[0m step \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m     60\u001b[0m \u001b[39mif\u001b[39;00m idx \u001b[39m%\u001b[39m \u001b[39m100\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m~/michaelliang/venv/lib/python3.8/site-packages/torch/optim/optimizer.py:140\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m profile_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mOptimizer.step#\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.step\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(obj\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)\n\u001b[1;32m    139\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mrecord_function(profile_name):\n\u001b[0;32m--> 140\u001b[0m     out \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    141\u001b[0m     obj\u001b[39m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    142\u001b[0m     \u001b[39mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/michaelliang/venv/lib/python3.8/site-packages/torch/optim/optimizer.py:23\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     22\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdefaults[\u001b[39m'\u001b[39m\u001b[39mdifferentiable\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m---> 23\u001b[0m     ret \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     24\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     25\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(prev_grad)\n",
      "File \u001b[0;32m~/michaelliang/venv/lib/python3.8/site-packages/torch/optim/adam.py:234\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure, grad_scaler)\u001b[0m\n\u001b[1;32m    231\u001b[0m                 \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39m`requires_grad` is not supported for `step` in differentiable mode\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    232\u001b[0m             state_steps\u001b[39m.\u001b[39mappend(state[\u001b[39m'\u001b[39m\u001b[39mstep\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m--> 234\u001b[0m     adam(params_with_grad,\n\u001b[1;32m    235\u001b[0m          grads,\n\u001b[1;32m    236\u001b[0m          exp_avgs,\n\u001b[1;32m    237\u001b[0m          exp_avg_sqs,\n\u001b[1;32m    238\u001b[0m          max_exp_avg_sqs,\n\u001b[1;32m    239\u001b[0m          state_steps,\n\u001b[1;32m    240\u001b[0m          amsgrad\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mamsgrad\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    241\u001b[0m          beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[1;32m    242\u001b[0m          beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[1;32m    243\u001b[0m          lr\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mlr\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    244\u001b[0m          weight_decay\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mweight_decay\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    245\u001b[0m          eps\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39meps\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    246\u001b[0m          maximize\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mmaximize\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    247\u001b[0m          foreach\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mforeach\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    248\u001b[0m          capturable\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mcapturable\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    249\u001b[0m          differentiable\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mdifferentiable\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    250\u001b[0m          fused\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mfused\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    251\u001b[0m          grad_scale\u001b[39m=\u001b[39;49mgrad_scale,\n\u001b[1;32m    252\u001b[0m          found_inf\u001b[39m=\u001b[39;49mfound_inf)\n\u001b[1;32m    254\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/michaelliang/venv/lib/python3.8/site-packages/torch/optim/adam.py:300\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    297\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    298\u001b[0m     func \u001b[39m=\u001b[39m _single_tensor_adam\n\u001b[0;32m--> 300\u001b[0m func(params,\n\u001b[1;32m    301\u001b[0m      grads,\n\u001b[1;32m    302\u001b[0m      exp_avgs,\n\u001b[1;32m    303\u001b[0m      exp_avg_sqs,\n\u001b[1;32m    304\u001b[0m      max_exp_avg_sqs,\n\u001b[1;32m    305\u001b[0m      state_steps,\n\u001b[1;32m    306\u001b[0m      amsgrad\u001b[39m=\u001b[39;49mamsgrad,\n\u001b[1;32m    307\u001b[0m      beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[1;32m    308\u001b[0m      beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[1;32m    309\u001b[0m      lr\u001b[39m=\u001b[39;49mlr,\n\u001b[1;32m    310\u001b[0m      weight_decay\u001b[39m=\u001b[39;49mweight_decay,\n\u001b[1;32m    311\u001b[0m      eps\u001b[39m=\u001b[39;49meps,\n\u001b[1;32m    312\u001b[0m      maximize\u001b[39m=\u001b[39;49mmaximize,\n\u001b[1;32m    313\u001b[0m      capturable\u001b[39m=\u001b[39;49mcapturable,\n\u001b[1;32m    314\u001b[0m      differentiable\u001b[39m=\u001b[39;49mdifferentiable,\n\u001b[1;32m    315\u001b[0m      grad_scale\u001b[39m=\u001b[39;49mgrad_scale,\n\u001b[1;32m    316\u001b[0m      found_inf\u001b[39m=\u001b[39;49mfound_inf)\n",
      "File \u001b[0;32m~/michaelliang/venv/lib/python3.8/site-packages/torch/optim/adam.py:363\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    360\u001b[0m     param \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mview_as_real(param)\n\u001b[1;32m    362\u001b[0m \u001b[39m# Decay the first and second moment running average coefficient\u001b[39;00m\n\u001b[0;32m--> 363\u001b[0m exp_avg\u001b[39m.\u001b[39;49mmul_(beta1)\u001b[39m.\u001b[39madd_(grad, alpha\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m \u001b[39m-\u001b[39m beta1)\n\u001b[1;32m    364\u001b[0m exp_avg_sq\u001b[39m.\u001b[39mmul_(beta2)\u001b[39m.\u001b[39maddcmul_(grad, grad\u001b[39m.\u001b[39mconj(), value\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m \u001b[39m-\u001b[39m beta2)\n\u001b[1;32m    366\u001b[0m \u001b[39mif\u001b[39;00m capturable \u001b[39mor\u001b[39;00m differentiable:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Tensorboard.\n",
    "writer_fake = SummaryWriter(f\"runs/GAN_MONET/fake\")\n",
    "writer_real = SummaryWriter(f\"runs/GAN_MONET/real\")\n",
    "\n",
    "# Optimizers\n",
    "opt_gen = torch.optim.Adam(list(gen_monet.parameters()) + list(gen_raw.parameters()), lr=lr)\n",
    "opt_disc = torch.optim.Adam(list(disc_monet.parameters()) + list(disc_raw.parameters()), lr=lr)\n",
    "\n",
    "# Loss\n",
    "L1 = nn.L1Loss() # Cycle consistency loss and identity loss\n",
    "mse = nn.MSELoss() # Adversarial loss\n",
    "\n",
    "step = 0\n",
    "# Training Loop\n",
    "for epoch in range(num_epochs):\n",
    "    for idx, (raw_img, monet_img) in enumerate(tqdm(dataloader)):\n",
    "        raw_img = raw_img.to(device)\n",
    "        monet_img = monet_img.to(device)\n",
    "        \n",
    "        # Train discriminators Monet and Raw\n",
    "        fake_monet = gen_monet(raw_img)\n",
    "        D_monet_real = disc_monet(monet_img)\n",
    "        D_monet_fake = disc_monet(fake_monet.detach())\n",
    "        D_monet_real_loss = mse(D_monet_real, torch.ones_like(D_monet_real))\n",
    "        D_monet_fake_loss = mse(D_monet_fake, torch.zeros_like(D_monet_fake))\n",
    "        D_monet_loss = D_monet_real_loss + D_monet_fake_loss\n",
    "\n",
    "        fake_raw = gen_raw(monet_img)\n",
    "        D_raw_real = disc_raw(raw_img)\n",
    "        D_raw_fake = disc_raw(fake_raw.detach())\n",
    "        D_raw_real_loss = mse(D_raw_real, torch.ones_like(D_raw_real))\n",
    "        D_raw_fake_loss = mse(D_raw_fake, torch.zeros_like(D_raw_fake))\n",
    "        D_raw_loss = D_raw_real_loss + D_raw_fake_loss\n",
    "\n",
    "        D_loss = (D_monet_loss + D_raw_loss)/2\n",
    "\n",
    "        opt_disc.zero_grad()\n",
    "        D_loss.backward(retain_graph=True)\n",
    "        opt_disc.step()\n",
    "\n",
    "        # Train generators Monet and Raw\n",
    "        D_monet_fake = disc_monet(fake_monet)\n",
    "        D_raw_fake = disc_raw(fake_raw)\n",
    "        loss_G_monet = mse(D_monet_fake, torch.ones_like(D_monet_fake)) # Adversarial loss\n",
    "        loss_G_raw = mse(D_raw_fake, torch.ones_like(D_raw_fake)) # Adversarial loss\n",
    "\n",
    "        # Cycle loss\n",
    "        cycle_monet = gen_monet(fake_raw)\n",
    "        cycle_raw = gen_raw(fake_monet)\n",
    "        cycle_monet_loss = L1(monet_img, cycle_monet)\n",
    "        cycle_raw_loss = L1(raw_img, cycle_raw)\n",
    "\n",
    "        G_loss = (loss_G_monet + loss_G_raw) + (cycle_monet_loss + cycle_raw_loss)\n",
    "        \n",
    "        opt_gen.zero_grad()\n",
    "        G_loss.backward()\n",
    "        opt_gen.step()\n",
    "\n",
    "        step += 1\n",
    "        if idx % 100 == 0:\n",
    "            print(f\"Epoch [{epoch}/{num_epochs}] Batch {idx}/{len(dataloader)} \\\n",
    "                  Loss D: {D_loss:.4f}, loss G: {G_loss:.4f}\")\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                fake = gen_monet(raw_img)\n",
    "                # Unnormalise the image\n",
    "                raw_img = raw_img.reshape(-1, 3, 256, 256)\n",
    "                raw_img = raw_img * 0.5 + 0.5\n",
    "                fake = fake.reshape(-1, 3, 256, 256)\n",
    "                fake = fake * 0.5 + 0.5\n",
    "                # Concatenate raw and fake images\n",
    "                img_grid = torch.cat((raw_img, fake), dim=0)\n",
    "                # Save image\n",
    "                save_image(img_grid, f\"outputs/monet_{step}.png\")\n",
    "       \n",
    "\n",
    "    # Can we somehow see what the images look like as we train on tensorboard?\n",
    "    # Save model\n",
    "    torch.save(gen_monet.state_dict(), f'outputs/gen_monet_{epoch}.pth')\n",
    "    torch.save(gen_raw.state_dict(), f'outputs/gen_raw_{epoch}.pth')\n",
    "    torch.save(disc_monet.state_dict(), f'outputs/disc_monet_{epoch}.pth')\n",
    "    torch.save(disc_raw.state_dict(), f'outputs/disc_raw_{epoch}.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13 (default, Apr 19 2022, 00:53:22) \n[GCC 7.5.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a73d05bede605b914e881cac473083bc67bbc1abfd934ac332bf311f6ebb9017"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
