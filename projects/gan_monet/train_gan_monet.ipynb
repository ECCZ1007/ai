{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args='find outputs/submission -mindepth 1 -delete', returncode=0)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Third-party imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "import glob\n",
    "import os\n",
    "from PIL import Image\n",
    "from typing import Callable, Optional\n",
    "import IPython\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.utils import save_image\n",
    "import subprocess\n",
    "import shutil\n",
    "\n",
    "# Device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = \"cpu\"\n",
    "\n",
    "# Make outputs folders\n",
    "os.makedirs(\"outputs\", exist_ok=True)\n",
    "subprocess.run(\"find outputs -mindepth 1 -delete\", shell=True)\n",
    "\n",
    "# Make submission directory\n",
    "os.makedirs(\"outputs/submission\", exist_ok=True)\n",
    "subprocess.run(\"find outputs/submission -mindepth 1 -delete\", shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and Model Hyperparameters\n",
    "lr = 3e-4\n",
    "batch_size = 16\n",
    "img_dim = 3 * 256 * 256\n",
    "num_epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\"Basic building block of the Generator and Discriminator networks.\n",
    "    \n",
    "    Consists of a linear layer, batch normalization and LeakyReLU activation.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels: int, out_channels: int, stride: int):\n",
    "        \"\"\"Initializes the Block.\n",
    "        \n",
    "        Args:\n",
    "            in_channels (int): Number of input features.\n",
    "            out_channels (int): Number of output features.\n",
    "            stride (int): Stride of conv block.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=4, stride=stride, padding=1, bias=True, padding_mode=\"reflect\"),\n",
    "            nn.InstanceNorm2d(out_channels),\n",
    "            nn.LeakyReLU(0.2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    \"\"\"Discriminator network.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels: int = 3, features: int = [64, 128, 256, 512]):\n",
    "        \"\"\"Initializes the Discriminator network.\n",
    "        \n",
    "        Args:\n",
    "            in_channels (int): Number of channels in the input image.\n",
    "            features (list): Number of features in each layer.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # Initial does not use instance norm in cycleGAN paper.\n",
    "        self.initial = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, features[0], kernel_size=4, stride=2, padding=1, padding_mode=\"reflect\"),\n",
    "            nn.LeakyReLU(0.2)\n",
    "        )\n",
    "        layers = []\n",
    "        in_channels = features[0]\n",
    "        for feature in features[1:]:\n",
    "            layers.append(Block(in_channels, feature, stride=1 if feature == features[-1] else 2))\n",
    "            in_channels = feature\n",
    "        layers.append(nn.Conv2d(in_channels, 1, kernel_size=4, stride=1, padding=1, padding_mode=\"reflect\"))\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.initial(x)\n",
    "        return torch.sigmoid(self.model(x))\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    \"\"\"Building block for generator.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, down=True, use_act=True, **kwargs):\n",
    "        \"\"\"Initializes the ConvBlock.\n",
    "        \n",
    "        Args:\n",
    "            in_channels (int): Number of input channels.\n",
    "            out_channels (int): Number of output channels.\n",
    "            down (bool): Whether to use a downsample or upsample.\n",
    "            use_act (bool): Whether to use an activation function.\n",
    "            **kwargs: Additional arguments for the convolutional layer.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, padding_mode=\"reflect\", **kwargs) if down else nn.ConvTranspose2d(in_channels, out_channels, **kwargs),\n",
    "            nn.InstanceNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True) if use_act else nn.Identity()\n",
    "        )\n",
    "        self.down = down\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    \"\"\"Residual block for generator.\n",
    "    \"\"\"\n",
    "    def __init__(self, channels):\n",
    "        \"\"\"Initializes the ResidualBlock.\n",
    "        \n",
    "        Args:\n",
    "            channels (int): Number of channels in the input and output.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            ConvBlock(channels, channels, kernel_size=3, stride=1, padding=1),\n",
    "            ConvBlock(channels, channels, use_act=False, kernel_size=3, stride=1, padding=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.block(x)\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    \"\"\"Generator network.\n",
    "    \n",
    "    Attempts to augoment an input image to look like a Monet painting.\n",
    "    \"\"\"\n",
    "    def __init__(self, img_channels: int = 3, num_features: int = 64, num_residuals: int = 9):\n",
    "        \"\"\"Initializes the Generator network.\n",
    "        \n",
    "        Args:\n",
    "            img_channels (int): Dimension of the image space.\n",
    "            num_features (int): Number of features to use.\n",
    "            num_residuals (int): Number of residual blocks to use.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.initial = nn.Sequential(\n",
    "            nn.Conv2d(img_channels, num_features, kernel_size=7, stride=1, padding=3),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.down_blocks = nn.ModuleList([\n",
    "            ConvBlock(num_features, num_features*2, down=True, kernel_size=3, stride=2, padding=1),\n",
    "            ConvBlock(num_features*2, num_features*4, down=True, kernel_size=3, stride=2, padding=1),\n",
    "        ])\n",
    "        self.residual_blocks = nn.Sequential(*[ResidualBlock(num_features*4) for _ in range(num_residuals)])\n",
    "        self.up_blocks = nn.ModuleList([\n",
    "            ConvBlock(num_features*4, num_features*2, down=False, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            ConvBlock(num_features*2, num_features, down=False, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "        ])\n",
    "        self.last = nn.Conv2d(num_features, img_channels, kernel_size=7, stride=1, padding=3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.initial(x)\n",
    "        for down_block in self.down_blocks:\n",
    "            x = down_block(x)\n",
    "        x = self.residual_blocks(x)\n",
    "        for up_block in self.up_blocks:\n",
    "            x = up_block(x)\n",
    "        return torch.tanh(self.last(x))\n",
    "\n",
    "# Init the models\n",
    "gen_monet = Generator().to(device)\n",
    "gen_raw = Generator().to(device)\n",
    "disc_monet = Discriminator().to(device)\n",
    "disc_raw = Discriminator().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datasets\n",
    "class ImageDataset(Dataset):\n",
    "    \"\"\"Image dataset.\n",
    "    \n",
    "    Loads images from a directory.\n",
    "    \"\"\"\n",
    "    def __init__(self, train_dir: str, test_dir: str, transforms: Optional[Callable] = None):\n",
    "        \"\"\"Initializes the ImageDataset.\n",
    "        \n",
    "        Args:\n",
    "            train_dir (str): Directory containing the raw training images.\n",
    "            test_dir (str): Directory containing the raw training images.\n",
    "            transforms (Optional[Callable]): Optional transform to be applied on a sample.\n",
    "        \"\"\"\n",
    "        self.train_dir = sorted(glob.glob(os.path.join(train_dir, \"*.jpg\")))\n",
    "        self.test_dir = sorted(glob.glob(os.path.join(test_dir, \"*.jpg\")))\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        train_image = Image.open(self.train_dir[index % len(self.train_dir)])\n",
    "        test_image = Image.open(self.test_dir[index % len(self.test_dir)])\n",
    "        if self.transforms:\n",
    "            train_image = self.transforms(train_image)\n",
    "            test_image = self.transforms(test_image)\n",
    "        return train_image, test_image\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.train_dir)\n",
    "# Image Transforms\n",
    "transforms = transforms.Compose([\n",
    "    transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "# Create the dataset\n",
    "ds = ImageDataset(train_dir='data/photo_jpg/', test_dir='data/monet_jpg/', transforms=transforms)\n",
    "\n",
    "# Create the data loader\n",
    "dataloader = DataLoader(ds, batch_size=batch_size, shuffle=True, num_workers=4)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/cirrascale/michaelliang/deep-learning/projects/gan_monet/submission.zip'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Optimizers\n",
    "opt_gen = torch.optim.Adam(list(gen_monet.parameters()) + list(gen_raw.parameters()), lr=lr)\n",
    "opt_disc = torch.optim.Adam(list(disc_monet.parameters()) + list(disc_raw.parameters()), lr=lr)\n",
    "\n",
    "# Loss\n",
    "L1 = nn.L1Loss() # Cycle consistency loss and identity loss\n",
    "mse = nn.MSELoss() # Adversarial loss\n",
    "\n",
    "step = 0\n",
    "# Training Loop\n",
    "for epoch in range(num_epochs):\n",
    "    for idx, (raw_img, monet_img) in enumerate(tqdm(dataloader)):\n",
    "        raw_img = raw_img.to(device)\n",
    "        monet_img = monet_img.to(device)\n",
    "        \n",
    "        # Train discriminators Monet and Raw\n",
    "        fake_monet = gen_monet(raw_img)\n",
    "        D_monet_real = disc_monet(monet_img)\n",
    "        D_monet_fake = disc_monet(fake_monet.detach())\n",
    "        D_monet_real_loss = mse(D_monet_real, torch.ones_like(D_monet_real))\n",
    "        D_monet_fake_loss = mse(D_monet_fake, torch.zeros_like(D_monet_fake))\n",
    "        D_monet_loss = D_monet_real_loss + D_monet_fake_loss\n",
    "\n",
    "        fake_raw = gen_raw(monet_img)\n",
    "        D_raw_real = disc_raw(raw_img)\n",
    "        D_raw_fake = disc_raw(fake_raw.detach())\n",
    "        D_raw_real_loss = mse(D_raw_real, torch.ones_like(D_raw_real))\n",
    "        D_raw_fake_loss = mse(D_raw_fake, torch.zeros_like(D_raw_fake))\n",
    "        D_raw_loss = D_raw_real_loss + D_raw_fake_loss\n",
    "\n",
    "        D_loss = (D_monet_loss + D_raw_loss)/2\n",
    "\n",
    "        opt_disc.zero_grad()\n",
    "        D_loss.backward(retain_graph=True)\n",
    "        opt_disc.step()\n",
    "\n",
    "        # Train generators Monet and Raw\n",
    "        D_monet_fake = disc_monet(fake_monet)\n",
    "        D_raw_fake = disc_raw(fake_raw)\n",
    "        loss_G_monet = mse(D_monet_fake, torch.ones_like(D_monet_fake)) # Adversarial loss\n",
    "        loss_G_raw = mse(D_raw_fake, torch.ones_like(D_raw_fake)) # Adversarial loss\n",
    "\n",
    "        # Cycle loss\n",
    "        cycle_monet = gen_monet(fake_raw)\n",
    "        cycle_raw = gen_raw(fake_monet)\n",
    "        cycle_monet_loss = L1(monet_img, cycle_monet)\n",
    "        cycle_raw_loss = L1(raw_img, cycle_raw)\n",
    "\n",
    "        G_loss = (loss_G_monet + loss_G_raw) + (cycle_monet_loss + cycle_raw_loss)\n",
    "        \n",
    "        opt_gen.zero_grad()\n",
    "        G_loss.backward()\n",
    "        opt_gen.step()\n",
    "\n",
    "        step += 1\n",
    "        if idx % 100 == 0:\n",
    "            print(f\"Epoch [{epoch}/{num_epochs}] Batch {idx}/{len(dataloader)} \\\n",
    "                  Loss D: {D_loss:.4f}, loss G: {G_loss:.4f}\")\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                fake = gen_monet(raw_img)\n",
    "                # Unnormalise the image\n",
    "                raw_img = raw_img.reshape(-1, 3, 256, 256)\n",
    "                raw_img = raw_img * 0.5 + 0.5\n",
    "                fake = fake.reshape(-1, 3, 256, 256)\n",
    "                fake = fake * 0.5 + 0.5\n",
    "                # Concatenate raw and fake images\n",
    "                img_grid = torch.cat((raw_img, fake), dim=0)\n",
    "                # Save image\n",
    "                save_image(img_grid, f\"outputs/monet_{step}.png\")\n",
    "       \n",
    "\n",
    "    # Can we somehow see what the images look like as we train on tensorboard?\n",
    "    # Save model\n",
    "    torch.save(gen_monet.state_dict(), f'outputs/gen_monet_{epoch}.pth')\n",
    "    torch.save(gen_raw.state_dict(), f'outputs/gen_raw_{epoch}.pth')\n",
    "    torch.save(disc_monet.state_dict(), f'outputs/disc_monet_{epoch}.pth')\n",
    "    torch.save(disc_raw.state_dict(), f'outputs/disc_raw_{epoch}.pth')\n",
    "\n",
    "# Build submission\n",
    "gen_monet.eval()\n",
    "\n",
    "for file in tqdm(sorted(glob.glob(os.path.join(\"data/photo_jpg/\", \"*.jpg\")))):\n",
    "    file_name = file.split(\"/\")[-1]\n",
    "    img = Image.open(file)\n",
    "    img = transforms(img)\n",
    "    img = img.unsqueeze(0)\n",
    "    img = img.to(device)\n",
    "    pred = gen_monet(img)\n",
    "    pred = pred.reshape(-1, 3, 256, 256)\n",
    "    pred = pred * 0.5 + 0.5\n",
    "    save_image(pred, f\"outputs/submission/{file_name}\")\n",
    "\n",
    "# Zip submission folder\n",
    "shutil.make_archive(\"images\", \"zip\", \"outputs/submission\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a73d05bede605b914e881cac473083bc67bbc1abfd934ac332bf311f6ebb9017"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
